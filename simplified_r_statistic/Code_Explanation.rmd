---
title: "Code & Data Processing used for Rt Estimator"
output: html_document
---

```{r setup, include=FALSE, echo=F, message=F, warning=F}
knitr::opts_chunk$set(echo = TRUE,
                      cache = T,
                      cache.path = "_cache/",
                      warning = F,
                      message = F)
library("furrr")

no_cores <- availableCores() - 1
plan(multicore, workers = no_cores)
```

```{r include=F}
knitr::read_chunk("Estimate_R_Functions.R")
source("Estimate_R_Functions.R")
```

This document shows the describes the code used to process raw data and estimate Rt values. First, estimating Rt is dependant on 3 things: 

1) **New cases per day**: This data is downloaded and cleaned from multiple data sources (i.e. New York Times, Covidtracking.com, and Covid.ourworldindata.org)
2) **The generation interval of the virus**: This is the amount of time in takes from when a primary case is infected to when a secondary case is infected. The generation interval can be observed via careful contact-tracing studies. In the web-app, we allow the user to control the mean and standard deviation of the generation interval within a bounds that are consistent with empiric studies (i.e. Nishiura et al, 2020)
3) **Method of Estimating Rt**: We can apply different methods to estimate the Rt from the number of new cases per day and the generation interval. We allow the user of the webapp to use any of 4 methods (described below). 

## Download Data

We download data tables that contain the number of new cases per day separated by geographic region (Country, US State, US County). 

```{r download_data, message=F, warning=F}
library("tidyverse")

#download county level data from NYT 
US_countydata <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv", col_types = "Dcccnn")

#download country level data from covid.ourworldindata.org
countrydata <- read_csv("https://covid.ourworldindata.org/data/ecdc/full_data.csv", col_types = "Dcnnnn")

#download US State level data from covidtracking.com
column_specs = cols(
    date = col_date(format = "%Y%m%d"),
    state = col_character(),
    positive = col_double(),
    negative = col_double(),
    pending = col_double(),
    hospitalizedCurrently = col_double(),
    hospitalizedCumulative = col_double(),
    inIcuCurrently = col_double(),
    inIcuCumulative = col_double(),
    onVentilatorCurrently = col_double(),
    onVentilatorCumulative = col_double(),
    recovered = col_double(),
    dataQualityGrade = col_character(),
    lastUpdateEt = col_datetime(format = "%m/%d/%Y %H:%M"),
    hash = col_character(),
    dateChecked = col_skip(),
    death = col_double(),
    hospitalized = col_double(),
    total = col_double(),
    totalTestResults = col_double(),
    posNeg = col_double(),
    fips = col_character(),
    deathIncrease = col_double(),
    hospitalizedIncrease = col_double(),
    negativeIncrease = col_double(),
    positiveIncrease = col_double(),
    totalTestResultsIncrease = col_double()
)

US_statedata <- read_csv("https://covidtracking.com/api/v1/states/daily.csv", col_types = column_specs)

```


## Standardize Data

Since we are dealing with tables from different sources, the structures of these tables are not indentical. Here, we standardize the tables.

```{r standardize_data, warning=F, message=F}
library(tigris)

# Create a unique label for each region in each dataset

US_countydata <- US_countydata %>%
    unite(col = region, county, state, sep=", ") %>%
    dplyr::select(date, region, regionID = fips, total_cases = cases) %>%
    mutate(region_type = "county", regionID_type = "fips")

US_statedata <- US_statedata %>%
    dplyr::select(date, region = state, total_cases = positive, new_cases = positiveIncrease, regionID = fips) %>%
    mutate(region_type = "state", regionID_type = "fips")
    countrydata <- countrydata %>%
    dplyr::select(date, region = location, new_cases, total_cases) %>%
    mutate(region_type = "nation", regionID = NA, regionID_type = NA)
    
#Replace abberviated names with full names in US State data
fips_reference = tigris::fips_codes %>% dplyr::select(1:3) %>% distinct()
    US_statedata = left_join(US_statedata, fips_reference, by=c("regionID" = "state_code")) %>%
    mutate(region = state_name) %>%
    mutate(region = paste(region, "United States", sep=", ")) %>%
    dplyr::select(-c(state, state_name))
```

<div id="bind_data">
Bind rows together
```{r bind_data}
DATA <- bind_rows(list(US_countydata, US_statedata, countrydata))
```
</div>

## Clean Data

<div id="clean_1">
For each region, remove leading dates that are just a series of 0s or NAs 

```{r clean_step1, warning=F, message=F}
DATA2 <- DATA %>%
  filter(!is.na(total_cases) & total_cases > 0) %>%
  group_by(region, regionID, region_type, regionID_type) %>%
  summarise(reference_date = min(date))

DATA <- full_join(DATA, DATA2) %>%
  filter(date >= reference_date) %>%
  dplyr::select(-reference_date)
```

</div>

<div id="clean_2">
Sometimes there are errors in the reporting of new cases and when the error is discovered, the cumulative number of cases decreases. To correct for this, for each region, remove dates that have cumulative number of cases is greater than a future date. 

```{r clean_step2, warning=F, message=F}

#Group data by region and arrange by date
DATA <- DATA %>%
  group_by(region, regionID, region_type, regionID_type) %>%
  arrange(date)

# Calculate new cases per day per region
DATA <- DATA %>% mutate(new_cases = c(0, diff(total_cases)))

# Create a column that will be used to mark dates that will be needed to interprolated (see next code block on interprolation)
DATES_TO_INTERPROLATE = filter(DATA, F) %>% ungroup()

DATA = DATA %>%
  # identify days with negative number of "new cases"
  mutate(negative_increase = new_cases < 0) %>%
  mutate(has_negative_newcases = any(negative_increase, na.rm=T))

while(any(DATA$new_cases < 0)) {
  DATA = split(DATA, DATA$has_negative_newcases)
  
  DATA[["TRUE"]] = DATA[["TRUE"]] %>%
    # identify when the latest date in which new_cases < 0
    mutate(reference_date = which.max(negative_increase)) %>%
    # identify days before reference_date that have more total_cases than reference_date
    mutate(rows_to_filter = (((date < date[reference_date]) & (total_cases > total_cases[reference_date]))))
  
  # add days you will filter out to DATES_TO_INTERPROLATE
  DATES_TO_INTERPROLATE = DATA[["TRUE"]] %>% filter(rows_to_filter == T) %>% bind_rows(DATES_TO_INTERPROLATE, .)
  
  DATA[["TRUE"]] = DATA[["TRUE"]] %>%
    # filter out days
    filter(rows_to_filter == F) %>%
    # recalculate new_cases
    mutate(new_cases = c(0, diff(total_cases)))
  
  DATA = bind_rows(DATA)
  
  DATA = DATA %>%
    # identify days with negative number of "new cases"
    mutate(negative_increase = new_cases < 0) %>%
    mutate(has_negative_newcases = any(negative_increase))
}

# Remove that are no longer necessary
DATA = DATA %>%
  dplyr::select(-c(negative_increase, has_negative_newcases, reference_date, rows_to_filter)) %>%
  ungroup()

#DT::datatable(DATA)
```

</div>

<div id="clean_3">
For the dates that are removed in the above step, we will interporlate the cumulative number of cases as linearly increasing between the two dates that have known cumulative case numbers.  

```{r clean_step3, message=F, warning=F}  
# -------------------------------------------------------
# Interprolate total_cases for dates that were removed
# -------------------------------------------------------
library(zoo)

DATES_TO_INTERPROLATE = DATES_TO_INTERPROLATE %>%
  dplyr::select(region, region_type, regionID, regionID_type, date) %>%
  mutate(INTERPROLATED = T)
  DATA = DATA %>% mutate(INTERPROLATED = F)

DATA = bind_rows(DATA, DATES_TO_INTERPROLATE)
DATA = DATA %>% dplyr::select(region, region_type, regionID, regionID_type, date, total_cases, new_cases, INTERPROLATED)

DATA = DATA %>%
  group_by(region, region_type, regionID, regionID_type) %>%
  arrange(date) %>%
  mutate(total_cases = zoo::na.approx(total_cases, na.rm=F)) %>%
  mutate(new_cases = c(0, diff(total_cases))) %>%
  ungroup()


# -------------------------------------------------------
# Replace NA values in new_cases with 0
# -------------------------------------------------------

DATA$new_cases = replace(DATA$new_cases, which(is.na(DATA$new_cases)), 0)

#DT::datatable(DATA)
```

</div>

```{r echo=F, message=F, warning=F}
write_csv(DATA, "case_data.csv")
```



## Select Region to Plot

```{r mesage=F, warning=F, echo=F}
geographic_location = "Ohio, United States"
smoothing_window = 7 
var.si_mean = 4
var.si_sd = 3
var.tau = 7
```

In the web app, the user decides on a region to analyze. For this demonstration, we'll have <code> geographic_location = "`r geographic_location`"</code> as the region to analyze

```{r message=F, warning=F}
data <-DATA %>% filter(region == geographic_location)
```

## Smooth Data

We use an unweighted rolling mean to smooth the number of new cases per day. In the web app, the user controls the window size (in days) for the rolling mean. Here we'll have <code> smoothing_window = `r smoothing_window`</code>. 

```{r Smoothing-Function}
```

```{r}
data = smooth_new_cases(data, smoothing_window)
```

<div id="estimate_r0">
## Estimate $R_t$

All the method here use the generation interval to estimate $R_t$. For the webapp we assume that the serial interval has a parametric distribution and the user can control the mean and standard deviation of the distribution (the exact type of distribution depends on the method used estimate $R_t$). Here, we'll assume the mean is `var.si_mean` = `r var.si_mean` and standard deviation is `var.si_sd` = `r var.si_sd` (as measured for SARS-CoV-2 by Nishiura et al, 2020).

### Simple Ratio Method:

For this method, we assume it is sufficient to just use the mean of the generation interval for estimating $R_t$. The $R_t$ is estimated as the ratio between new cases ($I$) on day $t + D$ and day $t$, where $D$ is the mean generation interval. To have less statistical noise, instead of calculating the ratio of new cases for two days ($t$ and $t-D$), we calculate the ratio new cases for two time periods of size $\tau$ centered on days $t$ and $t+D$ (the default value of $\tau$ in the web-app is `var.tau` = `r var.tau` days). To calculate a confidence interval for the $R_t$, we treat $R_t$ as the ratio of two poisson rates (i.e. the number of cases with the time period $\tau$ are poisson distributed) and use the Clopper & Pearson (1934) method to develop confidence intervals, as implemented in the <code> binom.test() </code> function in R.   

$$ \mathbb{E}[R_t] \approx \frac{\sum_{i = t - \frac{\tau}{2} + D}^{t + \frac{\tau}{2} + D}I_i}{\sum_{i = t \frac{\tau}{2}}^{t + \frac{\tau}{2}}I_i}$$

We do this in R with the following code: 

```{r Simple-R-Estimate}
```

```{r warning=F, message=F}

simpleR.df = EstimateR.simple(date = data$date, Is = data$new_cases_smoothed, si_mean = var.si_mean, tau = var.tau)

df = left_join(data, simpleR.df, by="date")

library(ggpubr)

ggplot(df) + 
  geom_ribbon(aes(x = date,  ymin=`Simple Ratio.R_Quantile_025`, ymax=`Simple Ratio.R_Quantile_975`, group = region), alpha=0.15) +
  geom_line(aes(x = date, y = `Simple Ratio.R_mean`, group = region), size =1.25) +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(subtitle = "Shaded Region gives 95% CI for each R estimate", y = "R estimate") +
  scale_x_date(date_breaks = '1 week', date_labels = '%b-%d') +
  theme_pubr() + 
  theme(axis.text.x = element_text(angle=45, hjust=1))  
```

###  Cori et al (2013) $R_t$ estimation

Here we use one subset of methods described by Cori et al (2013). For this particular method, the generation interval is assumed to be gamma distributed. The R value is also assumed to be gamma-distributed as follows: 

$$ R_t \sim \Gamma(shape = a + \sum_{s=t-\tau+1}^{t} I_s, scale = \frac{1}{\frac{1}{b} + \sum_{s=t-\tau+1}^{t} I_sw_s})$$

$$\mathbb{E}[R_t] = \frac{a + \sum_{s=t-\tau+1}^{t} I_s}{\frac{1}{b} + \sum_{s=t-\tau+1}^{t} I_sw_s} $$


Where $w_s$ is a probability distribution of how infectious individuals are at time $t$ given that they were infected $t-s$ days ago. The values of $a$ and $b$ are estimated using a Bayesian approach, with the priors of $a$ and $b$ being 1 and 5, respectively. We conduct these calculations using the EpiEstim package in R. 

```{r Cori-R-Estimate}
```

```{r warning=F, message=F}
coriR.df = EstimateR.cori(date = data$date, Is = data$new_cases_smoothed, si_mean = var.si_mean, si_sd = var.si_sd, tau = var.tau)

df = left_join(data, coriR.df, by="date")

ggplot(df) + 
  geom_ribbon(aes(x = date,  ymin=Cori.R_Quantile_025, ymax=Cori.R_Quantile_975, group = region), alpha=0.15) +
  geom_line(aes(x = date, y = Cori.R_mean, group = region), size =1.25) +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(subtitle = "Shaded Region gives 95% CI for each R estimate", y = "R estimate") +
  scale_x_date(date_breaks = '1 week', date_labels = '%b-%d') +
  theme_pubr() + 
  theme(axis.text.x = element_text(angle=45, hjust=1))  
```

### Wallinga & Teunis (2004)

This method is based on the idea that $R_t$ can be estimated from a network of pairs of cases. The likelihood that case $j$ infected case $i$ is $w(t_i - t_j)$ where $w(t)$ is the distribution of generation interval (i.e. the relative probability a case will transmit the infection $t$ units of time after the start of the infection) and $t_i$ and $t_j$ are the times at which cases $i$ and $j$ were infected, respectively. The relative likelihood that case $j$ infected case $i$ is $w(t_i - t_j)$ normalized to the probability of that any other case $k$ infected $i$: 

$$ p_{i,j} = \frac{w(t_i - t_j)}{\sum_{i \neq k}{w(t_i - t_k)}}$$
Thus, the effective reproduction value for case $j$ ($R_j$) is the sum of $p_{i,j}$ for all pairs $i$ and $j$. And the overall effective reproduction value ($R_t$) is the mean of all $R_j$ at a given time. 

$$R_j = \sum_{i} \frac{w(t_i - t_j)}{\sum_{i \neq k}{w(t_i - t_k)}}$$
$$ R_t = \bar{R_j} $$

We use the R0 package to estimate $R_t$ along with confidence intervals for the webapp. 

```{r WT-R-Estimate}
```

```{r message=F, warning=F}
WT.df = EstimateR.WT(date = data$date, Is = data$new_cases_smoothed, si_mean = var.si_mean, si_sd = var.si_sd)

df = left_join(data, WT.df, by="date")

ggplot(df) + 
  geom_ribbon(aes(x = date,  ymin=WT.R_Quantile_025, ymax=WT.R_Quantile_975, group = region), alpha=0.15) +
  geom_line(aes(x = date, y = WT.R_mean, group = region), size =1.25) +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(subtitle = "Shaded Region gives 95% CI for each R estimate", y = "R estimate") +
  scale_x_date(date_breaks = '1 week', date_labels = '%b-%d') +
  theme_pubr() + 
  theme(axis.text.x = element_text(angle=45, hjust=1))  
```


### Wallinga & Lipsitch (2007)

The Walling & Lipsitch (2007) describe determing $R_t$ in two steps: 

1) Determine the exponential growth rate ($r$). Here we determine $r$ at each unit of time ($t$) by using `glm` function to fit a value of $r$ on a time window (size $\tau$) centered on $t$. We assume infections within that time window are produced by a poisson function. 

2) You can calculate $R_t$ from $r$ by "plugging" $r$ into moment-generating function of the generation interval distribtion $w(t)$

$$ R_t = \frac{1}{\int_{t = 0}^{\infty} e^{-rt}w(t)dt}$$
This formula is implemented in the R0 package with the `R.from.r()` function. We produce confidence intervals of $R_t$ by applying the `R.from.r()` function to the confidence intervals of $r$ (produced by the glm function).

```{r WL-R-Estimate}
```

```{r message=F, warning=F}
WL.df = EstimateR.WL(date = data$date, Is = data$new_cases_smoothed, si_mean = var.si_mean, si_sd = var.si_sd, tau=7)

df = left_join(data, WL.df, by="date")

ggplot(df) + 
  geom_ribbon(aes(x = date,  ymin=WL.R_Quantile_025, ymax=WL.R_Quantile_975, group = region), alpha=0.15) +
  geom_line(aes(x = date, y = WL.R_mean, group = region), size =1.25) +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(subtitle = "Shaded Region gives 95% CI for each R estimate", y = "R estimate") +
  scale_x_date(date_breaks = '1 week', date_labels = '%b-%d') +
  theme_pubr() + 
  theme(axis.text.x = element_text(angle=45, hjust=1))  
```
</div>