---
title: "Code for R Estimator"
output: html_document
---

```{r setup, include=FALSE, echo=F, message=F, warning=F}
knitr::opts_chunk$set(echo = TRUE)
```

## Download Data

```{r download_data, message=F, warning=F}
library("tidyverse")

#download county level data from NYT 
US_countydata <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv", col_types = "Dcccnn")

#download country level data from covid.ourworldindata.org
countrydata <- read_csv("https://covid.ourworldindata.org/data/ecdc/full_data.csv", col_types = "Dcnnnn")

#download US State level data from covidtracking.com
column_specs = cols(
    date = col_date(format = "%Y%m%d"),
    state = col_character(),
    positive = col_double(),
    negative = col_double(),
    pending = col_double(),
    hospitalizedCurrently = col_double(),
    hospitalizedCumulative = col_double(),
    inIcuCurrently = col_double(),
    inIcuCumulative = col_double(),
    onVentilatorCurrently = col_double(),
    onVentilatorCumulative = col_double(),
    recovered = col_double(),
    dataQualityGrade = col_character(),
    lastUpdateEt = col_datetime(format = "%m/%d/%Y %H:%M"),
    hash = col_character(),
    dateChecked = col_skip(),
    death = col_double(),
    hospitalized = col_double(),
    total = col_double(),
    totalTestResults = col_double(),
    posNeg = col_double(),
    fips = col_character(),
    deathIncrease = col_double(),
    hospitalizedIncrease = col_double(),
    negativeIncrease = col_double(),
    positiveIncrease = col_double(),
    totalTestResultsIncrease = col_double()
)

US_statedata <- read_csv("https://covidtracking.com/api/v1/states/daily.csv", col_types = column_specs)

```


## Standardize Data

```{r standardize_data, warning=F, message=F}
library(tigris)

# Create a unique label for each region in each dataset

US_countydata <- US_countydata %>%
    unite(col = region, county, state, sep=", ") %>%
    dplyr::select(date, region, regionID = fips, total_cases = cases) %>%
    mutate(region_type = "county", regionID_type = "fips")

US_statedata <- US_statedata %>%
    dplyr::select(date, region = state, total_cases = positive, new_cases = positiveIncrease, regionID = fips) %>%
    mutate(region_type = "state", regionID_type = "fips")
    countrydata <- countrydata %>%
    dplyr::select(date, region = location, new_cases, total_cases) %>%
    mutate(region_type = "nation", regionID = NA, regionID_type = NA)
    
#Replace abberviated names with full names in US State data
fips_reference = tigris::fips_codes %>% dplyr::select(1:3) %>% distinct()
    US_statedata = left_join(US_statedata, fips_reference, by=c("regionID" = "state_code")) %>%
    mutate(region = state_name) %>%
    mutate(region = paste(region, "United States", sep=", ")) %>%
    dplyr::select(-c(state, state_name))
```

<div id="bind_data">
Bind rows together
```{r bind_data}
DATA <- bind_rows(list(US_countydata, US_statedata, countrydata))

#DT::datatable(DATA)
```
</div>

## Clean Data

<div id="clean_1">
For each region, remove leading dates that are just a series of 0s or NAs 

```{r clean_step1, warning=F, message=F}
DATA2 <- DATA %>%
  filter(!is.na(total_cases) & total_cases > 0) %>%
  group_by(region, regionID, region_type, regionID_type) %>%
  summarise(reference_date = min(date))

DATA <- full_join(DATA, DATA2) %>%
  filter(date >= reference_date) %>%
  dplyr::select(-reference_date)

#DT::datatable(DATA)
```

</div>

<div id="clean_2">
Sometimes there are errors in the reporting of new cases and when the error is discovered, the cumulative number of cases decreases. To correct for this, for each region, remove dates that have cumulative number of cases is greater than a future date. 

```{r clean_step2, warning=F, message=F}

#Group data by region and arrange by date
DATA <- DATA %>%
  group_by(region, regionID, region_type, regionID_type) %>%
  arrange(date)

# Calculate new cases per day per region
DATA <- DATA %>% mutate(new_cases = c(0, diff(total_cases)))

# Create a column that will be used to mark dates that will be needed to interprolated (see next code block on interprolation)
DATES_TO_INTERPROLATE = filter(DATA, F) %>% ungroup()

DATA = DATA %>%
  # identify days with negative number of "new cases"
  mutate(negative_increase = new_cases < 0) %>%
  mutate(has_negative_newcases = any(negative_increase, na.rm=T))

while(any(DATA$new_cases < 0)) {
  DATA = split(DATA, DATA$has_negative_newcases)
  
  DATA[["TRUE"]] = DATA[["TRUE"]] %>%
    # identify when the latest date in which new_cases < 0
    mutate(reference_date = which.max(negative_increase)) %>%
    # identify days before reference_date that have more total_cases than reference_date
    mutate(rows_to_filter = (((date < date[reference_date]) & (total_cases > total_cases[reference_date]))))
  
  # add days you will filter out to DATES_TO_INTERPROLATE
  DATES_TO_INTERPROLATE = DATA[["TRUE"]] %>% filter(rows_to_filter == T) %>% bind_rows(DATES_TO_INTERPROLATE, .)
  
  DATA[["TRUE"]] = DATA[["TRUE"]] %>%
    # filter out days
    filter(rows_to_filter == F) %>%
    # recalculate new_cases
    mutate(new_cases = c(0, diff(total_cases)))
  
  DATA = bind_rows(DATA)
  
  DATA = DATA %>%
    # identify days with negative number of "new cases"
    mutate(negative_increase = new_cases < 0) %>%
    mutate(has_negative_newcases = any(negative_increase))
}

# Remove that are no longer necessary
DATA = DATA %>%
  dplyr::select(-c(negative_increase, has_negative_newcases, reference_date, rows_to_filter)) %>%
  ungroup()

#DT::datatable(DATA)
```

</div>

<div id="clean_3">
For the dates that are removed in the above step, we will interporlate the cumulative number of cases as linearly increasing between the two dates that have known cumulative case numbers.  

```{r clean_step3, message=F, warning=F}  
# -------------------------------------------------------
# Interprolate total_cases for dates that were removed
# -------------------------------------------------------
library(zoo)

DATES_TO_INTERPROLATE = DATES_TO_INTERPROLATE %>%
  dplyr::select(region, region_type, regionID, regionID_type, date) %>%
  mutate(INTERPROLATED = T)
  DATA = DATA %>% mutate(INTERPROLATED = F)

DATA = bind_rows(DATA, DATES_TO_INTERPROLATE)
DATA = DATA %>% dplyr::select(region, region_type, regionID, regionID_type, date, total_cases, new_cases, INTERPROLATED)

DATA = DATA %>%
  group_by(region, region_type, regionID, regionID_type) %>%
  arrange(date) %>%
  mutate(total_cases = zoo::na.approx(total_cases, na.rm=F)) %>%
  mutate(new_cases = c(0, diff(total_cases))) %>%
  ungroup()


# -------------------------------------------------------
# Replace NA values in new_cases with 0
# -------------------------------------------------------

DATA$new_cases = replace(DATA$new_cases, which(is.na(DATA$new_cases)), 0)

#DT::datatable(DATA)
```

</div>

```{r echo=F, message=F, warning=F}
write_csv(DATA, "case_data.csv")
```



## Select Region to Plot

```{r mesage=F, warning=F, echo=F}
geographic_location = "Ohio, United States"
smoothing_window = 7 
var.si_mean = 4
var.si_sd = 3
var.tau = 7
```

For demonstration purpuoses, we'll select the `r geographic_location` as the region to analyze

```{r message=F, warning=F}

#geographic_location holds the user input on geographic location 
data <-DATA %>% filter(region %in% geographic_location)
```

## Smooth Data

We use an unweighted rolling mean to smooth the number of new cases per day. The the window size for the rolling mean can be user  controlled in the web app. Here we will use a smoothing window size of `r smoothing_window` days. 

```{r message=F, warning=F}
library(RcppRoll)

#smoothing_window hold the user input on the size of the smoothing window
data = data %>%
    group_by(region, region_type, regionID, regionID_type) %>%
    mutate(new_cases_smoothed = roll_mean(new_cases, n = smoothing_window, align="center", fill = c(NA, NA, NA), na.rm=T)) %>%
    mutate(new_cases_smoothed = replace(new_cases_smoothed, is.na(new_cases_smoothed), new_cases[is.na(new_cases_smoothed)])) %>%
    ungroup()


# replace NAs in new_cases_smoothed with value from new_cases
data$new_cases_smoothed[is.na(data$new_cases_smoothed)] = data$new_cases[is.na(data$new_cases_smoothed)]

# Remove smoothed new cases that result in NAs or leading 0s
data = data %>% filter(!is.na(new_cases_smoothed)) %>% 
  mutate(reference_date = date[min(which((new_cases_smoothed > 0)))]) %>%
  filter(date >= reference_date) %>%
  dplyr::select(-reference_date)

#DT::datatable(data)
```

<div id="estimate_r0">
## Estimate $R_t$

All the method here use a the serial interval (as a proxy of the generation interval) to estimate $R_t$. For the webapp we assume that the serial interval has a parametric distribution and the user can control the mean and standard deviation of the distribution (the exact type of distribution is depends on the method used estimate $R_t$). Here, we'll assume the mean is `r var.si_mean` and standard deviation of `r var.si_sd` (as measured for SARS-CoV-2 by Nishiura et al, 2020).
```{r include=F}
knitr::read_chunk("Estimate_R_Functions.R")
source("Estimate_R_Functions.R")
```

### Simple R calcuation

For this estimation method, the serial interval is assumed to be well-approximated by a point mass (the mean or median of the serial interval distribution). The $R_t$ is estimated as the ratio between new cases ($I$) on day $t + D$ and day $t$, where $D$ is the mean serial interval. To have less statistical noise, instead of calculating the ratio of new cases for two days ($t$ and $t-D$), we calculate the ratio new cases for two time periods of size \tau (by default we assume \tau = `r var.tau` days). To calculate a confidence interal for the R-value, we'll assume that R is gamma distributed with parameters $k$ and \theta. 

$$ R_t \sim \Gamma(shape = \sum_{s=t+D-\tau+1}^{t+D} I_s, scale = \frac{1}{\sum_{s=t-\tau+1}^{t} I_s})$$

$$ \mathbb{E}[R_t] \approx \frac{\sum_{s=t+D-\tau+1}^{t+D} I_s}{\sum_{s=t-\tau+1}^{t-D} I_s}$$

We do this in R with the following code: 

```{r Simple-R-Estimate}
```

```{r warning=F, message=F}

simpleR.df = EstimateR.simple(date = data$date, Is = data$new_cases_smoothed, si_mean = var.si_mean, tau = var.tau)

df = left_join(data, simpleR.df, by="date")

library(ggpubr)

ggplot(df) + 
  geom_ribbon(aes(x = date,  ymin=KN.R_Quantile_025, ymax=KN.R_Quantile_975, group = region), alpha=0.15) +
  geom_line(aes(x = date, y = KN.R_mean, group = region), size =1.25) +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(subtitle = "Shaded Region gives 95% CI for each R estimate", y = "R estimate") +
  scale_x_date(date_breaks = '1 week', date_labels = '%b-%d') +
  theme_pubr() + 
  theme(axis.text.x = element_text(angle=45, hjust=1))  
```

###  Cori et al (2013) $R_t$ estimation

Here we use one subset of methods described by Cori et al (2013). For this particular method, the serial interval is assumed to be gamma distributed. The R value is also assumed to be gamma-distributed as follows: 

$$ R_t \sim \Gamma(shape = a + \sum_{s=t-\tau+1}^{t} I_s, scale = \frac{1}{\frac{1}{b} + \sum_{s=t-\tau+1}^{t} I_sw_s})$$

$$\mathbb{E}[R_t] = \frac{a + \sum_{s=t-\tau+1}^{t} I_s}{\frac{1}{b} + \sum_{s=t-\tau+1}^{t} I_sw_s} $$


Where $w_s$ is a probability distribution of how infectious individuals are at time $t$ given that they were infected $t-s$ days ago. The values of $a$ and $b$ are estimated using a Bayesian approach, with the priors of $a$ and $b$ being 1 and 5, respectively. We conduct these calculations using the EpiEstim package in R. 

```{r Cori-R-Estimate}
```

```{r warning=F, message=F}
coriR.df = EstimateR.cori(date = data$date, Is = data$new_cases_smoothed, si_mean = var.si_mean, si_sd = var.si_sd, tau = var.tau)

df = left_join(data, coriR.df, by="date")

ggplot(df) + 
  geom_ribbon(aes(x = date,  ymin=Cori.R_Quantile_025, ymax=Cori.R_Quantile_975, group = region), alpha=0.15) +
  geom_line(aes(x = date, y = Cori.R_mean, group = region), size =1.25) +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(subtitle = "Shaded Region gives 95% CI for each R estimate", y = "R estimate") +
  scale_x_date(date_breaks = '1 week', date_labels = '%b-%d') +
  theme_pubr() + 
  theme(axis.text.x = element_text(angle=45, hjust=1))  
```

### Wallinga & Teunis (2004)

This method is based on the idea that $R_t$ can be estimated from a network of pairs of cases. The likelihood that case $j$ infected case $i$ is $w(t_i - t_j)$ where $w(t)$ is the distribution of generation interval (i.e. the relative probability a case will transmit the infection $t$ units of time after the start of the infection) and $t_i$ and $t_j$ are the times at which cases $i$ and $j$ were infected, respectively. The relative likelihood that case $j$ infected case $i$ is $w(t_i - t_j)$ normalized to the probability of that any other case $k$ infected $i$: 

$$ p_{i,j} = \frac{w(t_i - t_j)}{\sum_{i \neq k}{w(t_i - t_k)}}$$
Thus, the effective reproduction value for case $j$ ($R_j$) is the sum of $p_{i,j}$ for all pairs $i$ and $j$. Then the effective reproduction value ($R_t$) is the mean of all $R_j$ at a given time. 

$$R_j = \sum_{i} \frac{w(t_i - t_j)}{\sum_{i \neq k}{w(t_i - t_k)}}$$

Wallinga & Teunis (2004) describes using a maximum-likelihood estimation to determine which infections network (and thus pairs of $i$ and $j$) has the highest likelihood of producing the observed epidemic curve. Details can be found in their [publication](https://academic.oup.com/aje/article/160/6/509/79472). We use the R0 package to estimate $R_t$ and a confidence interval for webapp, as follows: 

```{r WT-R-Estimate}
```

```{r message=F, warning=F}
WT.df = EstimateR.WT(date = data$date, Is = data$new_cases_smoothed, si_mean = var.si_mean, si_sd = var.si_sd)

df = left_join(data, WT.df, by="date")

ggplot(df) + 
  geom_ribbon(aes(x = date,  ymin=TD.R_Quantile_025, ymax=TD.R_Quantile_975, group = region), alpha=0.15) +
  geom_line(aes(x = date, y = TD.R_mean, group = region), size =1.25) +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(subtitle = "Shaded Region gives 95% CI for each R estimate", y = "R estimate") +
  scale_x_date(date_breaks = '1 week', date_labels = '%b-%d') +
  theme_pubr() + 
  theme(axis.text.x = element_text(angle=45, hjust=1))  
```


### Wallinga & Lipsitch (2007)

The Walling & Lipsitch (2007) describe determing $R_t$ in two steps: 

1) Determine the exponential growth rate ($r$). Here we determine $r$ at each unit of time ($t$) by using `glm` function to fit a value of $r$ on a time window (size $\tau$) centered on $t$. We assume infections within that time window are produced by a poisson function. 

2) You can calculate $R_t$ from $r$ by "plugging" $r$ into moment-generating function of the generation interval $w(t)$

$$ R_t = \frac{1}{\int_{t = 0}^{\infty} e^{-rt}w(t)dt}$$
This formula is implemented in the R0 package with the `R.from.r()` function. We produce confidence intervals of $R_t$ from the confidence intervals of $r$ produced by the glm function.

```{r WL-R-Estimate}
```

```{r message=F, warning=F}
WL.df = EstimateR.WL(date = data$date, Is = data$new_cases_smoothed, si_mean = var.si_mean, si_sd = var.si_sd, tau=7)

df = left_join(data, WL.df, by="date")

ggplot(df) + 
  geom_ribbon(aes(x = date,  ymin=WL.R_Quantile_025, ymax=WL.R_Quantile_975, group = region), alpha=0.15) +
  geom_line(aes(x = date, y = WL.R_mean, group = region), size =1.25) +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(subtitle = "Shaded Region gives 95% CI for each R estimate", y = "R estimate") +
  scale_x_date(date_breaks = '1 week', date_labels = '%b-%d') +
  theme_pubr() + 
  theme(axis.text.x = element_text(angle=45, hjust=1))  
```
</div>