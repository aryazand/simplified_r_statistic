---
title: "Results"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
header-includes:
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(message = F, 
                      warning = F)
```

## A Simple Ratio for Estimating $R_t$

If we assume that majority of secondary infection caused by a primary case occur at the mean generation interval of the infection ($D$), then the $R_t$ on day $t$ can be estimated as the ratio of the number of new cases ($I$) on day $t+D$ and day $t$.

$$R_t \approx \frac{I_{t+D}}{I_t}$$

However, reporting of new cases easily violates the assumption that the majority of secondary infections occur $D$ days afte the primary infection for two reasons. First, the actual transmission of the virus doesn't conform to a rigid schedule. Two, there is inevitable variance in the timing of when new cases are reported (e.g. new cases are typically reported less over weekends). We can handle these two violations of the assumption by estimating $R_t$ as the ratio of the sums of new cases within two time periods ( size $\tau$ days, with the center of the time periods being on days $t$ and $t+D$). By considering the sum of new cases within larger time periods (e.g. 7 days), we capture secondary infections that didn't occur exactly $D$ days from the primary infection and we also counteract variation in new case reporting. 

$$ R_t \approx \frac{\sum_{t - \frac{\tau}{2} + D}^{t + \frac{\tau}{2} + D}I_i}{\sum_{t - \frac{\tau}{2}}^{t + \frac{\tau}{2}}I_i}$$
<!---
Since it is nearly impossible for any public health programs to identify all new cases of infection each day, the reported new cases represent only a subset of the total cases. For estimation $R_t$ to be accurate, it depends on how this subset is selected in a few ways. First, the porportion of the total represented by this subset cannot change drastically between the two reporting periods (e.g. if the porportion of the population that is tested drastically increases between the two reporting periods, then $R_t$ will appear artificially inflated). Second, the demographics that influence susceptibility to the disease (or testing positive) of the people reported cannot change drastically between the reporting periods (e.g. only testing the elderly in one time period, and then switching to testing non-elderly will likely affect $R_t$ estimations). Third, ultimately the change in cases of the subset needs to reflect the rate of change of the entire population (e.g. if...)
--->

## The $R_t$ Estimator Web App

We developed a Shiny web-app that allows users to estimate $R_t$ for the SARS-CoV-2 pandemic in all countries, all states in the United States,, and all counties in the United States over time with 95% confidence intervals (or credible intervals) for the estimations. The user can estimate R using 4 different methods, all based on new cases per day and the generation interval (Table 1.) 

**Table 1. Methods for Estimating $R_t$**. 

```{r echo=F}
library("kableExtra")
library("tidyverse")
df = read_csv("table_1 - Description of Rt Estimation Methods.csv")
kable(df, "latex", align="l", booktabs=TRUE, escape = F) %>%
  kable_styling(full_width = F)%>%
  column_spec(1, width = "8em", bold = T, color = "black")%>%
  column_spec(2, width = "20em") %>%
  column_spec(3, width = "8em") %>%
  column_spec(4, width = "8em")
```


The user is able to select one or more geographic regions, and view a graph of how the $R_t$ changes over time for each of those regions. The user can modify parameters associated with the $R_t$ calculation, such as specifying the size of the smoothing window (the number days used for a rolling mean) to smooth the number of new cases per day, the mean and standard deviation of the generation interval, and the number of days to consider when calculating $R_t$ for each day. Additionally, the user can view the cumulative number of cases and new cases per day for each region selected. 

For each geographic region select, the web-app summarizes the the last 3 weeks of $R_t$ into colored score cards. For each week, the score card displays the range of $R_t$ point estimates. The color is based on the average limits of the confidence interval (CI) of $R_t$ for a particular week: the mean upper limit is below 1 (green); the mean point estimate is below 1 but the mean upper limit is above 1 (yellow); the mean point estimate is above 1 but the mean lower limit is below 1 (orange); the mean lower limit is above 1 (red). 

![](Figure_1.png)

**Figure 1. Interface of the Web-App**. The user selects one or more regions and a date-range of interest (A, red shaed region). For each of those selected regions and date-range, the $R_t$ value is displayed over time as a line graph on the top graph in the middle pane (B).  Additionally, for the lower graph, the user can toggle between viewing the cumulative number cases and new cases per day for each of the selected regions (C). The user controls parameters for calculating R: namely the size of the smoothing window for smoothing new cases per day,  method for calculating R, the mean and standard deviation of the generation interval, and the number of days used to calculate $R_t$ (D, green-shaded area). For each of the selected regions, a colored score card is displayed for each of the last three weeks based on the average $R_t$ and confidence interval of the estimations (E). If the user hovers over the graphs, additional information about the data is displayed in an information window (F). The date that the data and web-app were last updated are displayed (G). 

## Comparisons of $R_t$ Esimation Methods

```{r echo = F, message=F, warning=F} 
library("tidyverse")
library("RcppRoll")
```

> **Box 1. Summary of comparison** <br>
> Using the data for US States, the 4 different methods result in very similar estimates of $R_t$. In particular when: <br>
> 1. Smoothing window is increased <br>
> 2. The number of days for assessing the time-period <br>
> 3. When the mean and standard deviation of generation interval are low <br>


```{r echo=F, message = F, warning = F}

df= read_csv("DATA - baseline_parameters.csv")

df = df %>%  mutate(Comparison = factor(Comparison, levels = c("Simple Ratio vs Cori", "Simple Ratio vs Wallinga & Tuenis", "Simple Ratio vs Wallinga & Lipsitch", "Cori vs Wallinga & Teunis", "Cori vs Wallinga & Lipsitch", "Walinga & Teunis vs Wallinga & Lipsitch")))
```

When using the web-app, visual inspection shows that there is a great deal of agreement between the different methods of estimating $R_t$ (Figure 1 - add in a screenshot). We can quantify the difference in $R_t$ estimations using the absolute error. Here, we compared the $R_t$ values for each day of pandemic for 10 randomly selected Nations, US States, US Counties using the default parameters ^[smoothing window of 7 days, generation interval with mean of 4 day and standard deviation of 3 days, and time window for $R_t$ calculation of 7 days.], and we find very similar estimates of $R_t$ between the 4 methods - with the median error between any two estimates being `r round(median(df$RMSE, na.rm=T),3)` (Figure 2). 

We next assessed how the absolute error in estimation are affected by inputs and parameters for each estimation method. First, we find that as the number of new cases increases, the absolute error in $R_t$ estimations between the different methods decreases (Figure 3). Similarly, the absolute error between the $R_t$ estimations decrease if the size of the smoothing window is increased or if the number of days used to calculate $R_t$ is increased (Figures 4, S1, S2). An important parameters when caculating $R_t$ is the distribution of the generation interval. We find as the mean or standard deviation in the generation interval increased, there is also increase in the median absolute error of the $R_t$ estimations between the four methods (Figures 5, S3).

```{r echo=F, message = F, warning = F, fig.cap="Figure 2. Distribution of absolute error between all pairs of methods for estimating R", fig.width=12}

data_RMSE_stats <- df %>%
  filter(!is.infinite(RMSE)) %>%
  group_by(Comparison) %>%
  summarise(Mean = mean(RMSE, na.rm=T), 
            Median = median(RMSE, na.rm=T),
            `95_quantile` = quantile(RMSE, 0.95, na.rm=T))

library("ggpubr")
p <- ggplot(df) + 
  geom_histogram(aes(RMSE), fill="white", color="black") +
  geom_vline(data = data_RMSE_stats, aes(xintercept = Median), linetype = 2, color="blue") +
  geom_vline(data = data_RMSE_stats, aes(xintercept = `95_quantile`), linetype = 2, color="red") +
  labs(x = "Absolute Error") + 
  scale_x_log10(labels = scales::comma, limits=c(0.0001, 100)) +
  facet_wrap(~Comparison) + 
  theme_pubr()

q = ggplot_build(p)
ylim = q$layout$panel_scales_y[[1]]$range$range[[2]]

p + geom_text(data = data_RMSE_stats, 
               aes(x = Median, label = paste("50th-%tile:\n", round(Median,2))), 
               y = ylim*0.8, nudge_x=-1, color="blue") + 
    geom_text(data = data_RMSE_stats, 
               aes(x = `95_quantile`, label = paste("95th-%tile:\n", round(`95_quantile`,2))), 
               y = ylim*0.8, nudge_x=1, color="red")
```

```{r echo=F, message = F, warning = F, fig.cap = "Figure 3. The absolute error decreases as incidence increases.", fig.width=12}
library("ggpubr")

df <- df %>%
  group_by(region) %>%
  arrange(date) %>%
  mutate(weekly_sum = roll_sum(new_cases, n=7, align="right", fill = c(NA, NA, NA)))

df %>% filter(RMSE < 100) %>%
ggplot() +  
  geom_point(aes(new_cases_smoothed, RMSE), alpha=0.2) +
  facet_wrap(~Comparison, scales="free") +
  labs(x = "New cases per day", y = "Absolute Error") + 
  scale_y_log10(labels = scales::comma) + scale_x_log10(labels = scales::comma)

```

```{r echo=F, message = F, warning = F, fig.cap = "Figure 4. Median absolute error decreases as the size of the smoothing window or days used to calculate $R_t$ ($\\tau$) are increased", fig.width=12}

#---------------
# Smoothing Window
#----------------
df.sw = read_csv("DATA - vary_smoothing_parameter.csv")
df.sw = df.sw %>%  mutate(Comparison = factor(Comparison, levels = c("Simple Ratio vs Cori", "Simple Ratio vs Wallinga & Tuenis", "Simple Ratio vs Wallinga & Lipsitch", "Cori vs Wallinga & Teunis", "Cori vs Wallinga & Lipsitch", "Walinga & Teunis vs Wallinga & Lipsitch")))

df <- df.sw %>% group_by(smoothing_window, Comparison) %>%
  summarise(Median = median(RMSE, na.rm=T),
            `95th_percentile` = quantile(RMSE, 0.95, na.rm=T))

df$smoothing_window = as.numeric(df$smoothing_window)

df = df %>% pivot_longer(cols = c("Median", "95th_percentile"), names_to = "error_type", values_to = "error")
df$error_type =  df$error_type %>% fct_recode(`Median Error` = "Median",
                                              `95th Percentile of Error` = "95th_percentile")

a = df %>% filter(error_type == "Median Error") %>% 
      ggplot() +
      geom_line(aes(smoothing_window, error, color=Comparison)) +
      labs(y = "Median Absolute Error", x = "Smooth Window Size (days)") +
      theme_pubr()

#---------------
# Tau
#----------------
      
df.tau = read_csv("DATA - vary_tau_parameter.csv")
df.tau = df.tau %>%  mutate(Comparison = factor(Comparison, levels = c("Simple Ratio vs Cori", "Simple Ratio vs Wallinga & Tuenis", "Simple Ratio vs Wallinga & Lipsitch", "Cori vs Wallinga & Teunis", "Cori vs Wallinga & Lipsitch", "Walinga & Teunis vs Wallinga & Lipsitch")))

df <- df.tau %>% group_by(tau, Comparison) %>%
  summarise(Median = median(RMSE, na.rm=T),
            `95th_percentile` = quantile(RMSE, 0.95, na.rm=T))

df$tau = as.numeric(df$tau)

df = df %>% pivot_longer(cols = c("Median", "95th_percentile"), names_to = "error_type", values_to = "error")
df$error_type =  df$error_type %>% fct_recode(`Median Error` = "Median",
                                              `95th Percentile of Error` = "95th_percentile")

b = df %>% filter(error_type == "Median Error") %>% ungroup() %>%
      ggplot() +
      geom_line(aes(tau, error, group=Comparison, color=Comparison)) +
      labs(y = "Median Absolute Error", x = "Tau (days)") +
      theme_pubr() 
      
ggarrange(a,b, ncol=2, labels = c("A", "B"), common.legend=T)
```

```{r echo=F, message = F, warning = F, fig.cap = "Figure 5. The median absolute error tends to increase as the mean and standard deviation of the generation interval are increased.", fig.width=8, fig.height=6}

df.gt = read_csv("DATA - vary_GT_parameter.csv")
df.gt = df.gt %>%  mutate(Comparison = factor(Comparison, levels = c("Simple Ratio vs Cori", "Simple Ratio vs Wallinga & Tuenis", "Simple Ratio vs Wallinga & Lipsitch", "Cori vs Wallinga & Teunis", "Cori vs Wallinga & Lipsitch", "Walinga & Teunis vs Wallinga & Lipsitch")))

df <- df.gt %>% group_by(GT_mean, GT_SD, Comparison) %>% summarise(Median = median(RMSE, na.rm=T),
                                                    `95th_percentile` = quantile(RMSE, 0.95, na.rm=T))

p = ggplot(df) +
  geom_raster(aes(x=GT_mean, y=GT_SD, fill=Median)) +
  labs(x = "Mean", y="SD") +
  facet_wrap(~Comparison) + 
  scale_x_continuous(breaks = c(2,4,6,8,10)) + 
  scale_y_continuous(breaks = c(2,4,6,8,10)) + 
  scale_fill_continuous(type = "viridis") +
  theme_pubr()

p
```


```{r echo=F, message = F, warning = F, fig.cap = "Figure S1. The absolute error decreases as the size of the smoothing window is increased.", fig.width=12}

ylims = boxplot.stats(df.sw$RMSE)$stats[c(1, 5)]
df.sw %>% ggplot() + 
  geom_boxplot(aes(smoothing_window, RMSE, group=smoothing_window)) + 
  facet_wrap(~Comparison) + 
  coord_cartesian(ylim = ylims*1.5)
```


```{r echo=F, message = F, warning = F, fig.cap = "Figure S2. The absolute error decreases as the number of days used to calculate $R_t$ is increased.", fig.width=12}

ylims = boxplot.stats(df.tau$RMSE)$stats[c(1, 5)]
df.tau %>% ggplot() + 
  geom_boxplot(aes(tau, RMSE, group=tau)) + 
  facet_wrap(~Comparison) + 
  coord_cartesian(ylim = ylims*1.5)
```


```{r echo=F, message = F, warning = F, fig.cap = "Figure S3. The absolute error tends to increase as the mean and standard deviation of the generation interval are increased.", fig.width=12, fig.height=12}

p <-df.gt %>% 
  mutate(GT_mean = as.factor(GT_mean)) %>%
  mutate(GT_SD = paste("SD =", GT_SD)) %>% mutate(GT_SD = factor(GT_SD, levels = c("SD = 2", "SD = 4", "SD = 6", "SD = 8","SD = 10"))) %>%
  ggplot() + 
    geom_density(aes(RMSE, color=GT_mean, group=GT_mean), fill=NA) +
    labs(x = "Absolute Error") + 
    scale_x_log10(labels = scales::comma, limits=c(0.0001, 100)) +
    facet_grid(GT_SD~Comparison) + 
    theme_pubr()

p
```
